**SQL**


Table: Products

+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| product_id  | int     |
| low_fats    | enum    |
| recyclable  | enum    |
+-------------+---------+
product_id is the primary key (column with unique values) for this table.
low_fats is an ENUM (category) of type ('Y', 'N') where 'Y' means this product is low fat and 'N' means it is not.
recyclable is an ENUM (category) of types ('Y', 'N') where 'Y' means this product is recyclable and 'N' means it is not.
 

Write a solution to find the ids of products that are both low fat and recyclable.

Return the result table in any order.

The result format is in the following example.

 

Example 1:

Input: 
Products table:
+-------------+----------+------------+
| product_id  | low_fats | recyclable |
+-------------+----------+------------+
| 0           | Y        | N          |
| 1           | Y        | Y          |
| 2           | N        | Y          |
| 3           | Y        | Y          |
| 4           | N        | N          |
+-------------+----------+------------+
Output: 
+-------------+
| product_id  |
+-------------+
| 1           |
| 3           |
+-------------+
Explanation: Only products 1 and 3 are both low fat and recyclable.



Solutions

**SQL_Code**

select product_id 
from Products 
where low_fats = 'Y' and recyclable ='Y'


**PySpark_Code**
=========

from pyspark.sql.functions import *
from pyspark.sql.types import *

Products=[(0,'Yes','No'),
(1,'Yes','Yes'),
(2,'No','Yes' ),
(3,'Yes','Yes'),
(4,'No','No')]
Products_schema=["Products_id","low_fats","recyclable"]
Products_df=spark.createDataFrame(data=Products,schema=Products_schema)

Products_df.show()


Products_df.filter((col("low_fats")=="Yes") & (col("recyclable")=="Yes"))\
           .select(col("Products_id")).show()

















